{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfsuonrWtD-3"
   },
   "source": [
    "# Introduction to Natural Language Processing: A Journey through Next Word Prediction\n",
    "\n",
    "Welcome to the fascinating realm of Natural Language Processing (NLP), a field where language, technology, and human cognition intersect. Here, we embark on a journey to explore one of the most intriguing aspects of NLP: next word prediction. This objective is not just a cornerstone in understanding NLP, but also a window into the intricate ways machines can learn to interpret and generate human language.\n",
    "\n",
    "Every day, we interact with technology that anticipates what weâ€™re going to say next - be it while typing an email, texting, or searching online. This seemingly magical ability of machines to predict the next word in a sentence is a product of NLP. But how does it work? The answer lies in the ability of algorithms to learn from vast amounts of text data and understand patterns in language use.\n",
    "\n",
    "Next word prediction is more than a tool for faster typing. It's a fundamental technology in a variety of applications - from enhancing accessibility in communication aids to powering chatbots and virtual assistants. Its implications span various domains, including accessibility, automation, and artificial intelligence, making it a crucial subject in the study of NLP.\n",
    "\n",
    "At the heart of next word prediction lies the challenge of dealing with the complexity and variability of language. Here, we introduce a probabilistic framework to think about next word prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgQjpoTJlha6"
   },
   "source": [
    "## Quick Probability Review\n",
    "Probability is a mathematical framework for quantifying uncertainty. It is a way of thinking about the likelihood of events. In the context of NLP, we can use probability to think about the likelihood of a word occurring in a sentence. For example, the probability of the word \"cat\" occurring in the sentence \"the cat is on the mat\" is 1/5, or 0.2.\n",
    "\n",
    "## Conditional Probability\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. For example, the probability of the word \"cat\" occurring immediately after the word \"the\" is 1/2, or 0.5. \n",
    "\n",
    "$P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "We can write this as P(cat|the) = 0.5.\n",
    "\n",
    "Why? Because there are two words that can follow \"the\" in this sentence: \"cat\" and \"mat\". Since there is only one \"cat\" in the sentence, the probability of \"cat\" occurring after \"the\" is 1/2.\n",
    "\n",
    "## Joint Probability\n",
    "Joint probability is the probability of two events occurring together. For example, the probability of the word \"cat\" occurring in the sentence \"the cat is on the mat\" is 1/5, or 0.2. The probability of the word \"mat\" occurring in the same sentence is 1/5, or 0.2. The joint probability of \"cat\" and \"mat\" occurring together in the sentence is 1/25, or 0.04.\n",
    "\n",
    "$\\mathbb{P}(A \\cap B)=\\mathbb{P}(B \\mid A) \\mathbb{P}(A)=\\frac{1}{5} \\cdot \\frac{1}{5}=\\frac{1}{25}$\n",
    "\n",
    "## Chain Rule of Probability\n",
    "The chain rule of probability is a formula for calculating the joint probability of multiple events. For example, the probability of the words \"cat\" and \"mat\" occurring together in the sentence \"the cat is on the mat\" is 1/25, or 0.04. The probability of the word \"the\" occurring in the same sentence is 2/5, or 0.4. The chain rule of probability states that the joint probability of \"cat\", \"mat\", and \"the\" occurring together in the sentence is equal to the product of the conditional probabilities of each word given the words that came before it. In this case, the joint probability is equal to 0.04 * 0.4 = 0.016.\n",
    "\n",
    "$\\begin{aligned}\n",
    "P\\left(X_1 \\ldots X_n\\right) & =P\\left(X_1\\right) P\\left(X_2 \\mid X_1\\right) P\\left(X_3 \\mid X_{1: 2}\\right) \\ldots P\\left(X_n \\mid X_{1: n-1}\\right) \\\\\n",
    "& =\\prod_{k=1}^n P\\left(X_k \\mid X_{1: k-1}\\right)\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCIfBKJbnUO5"
   },
   "source": [
    "## Building a Next Word Predictor\n",
    "\n",
    "<b>Let's start with a simple probability exercise. Consider the corpus shown below. It consists of 3 sentences contained in a list. Based purely on counting, write a function to compute the probability of any given word in the corpus. This should be a global probability, i.e. the probability of a word occurring anywhere in the corpus.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "YTkz71lssV0A"
   },
   "outputs": [],
   "source": [
    "corpus = ['this is sentence one',\n",
    "          'this is sentence two',\n",
    "          'this is sentence three']\n",
    "\n",
    "\n",
    "def prob(word, corpus):\n",
    "    tokenized_corpus = [word.lower().strip('.,') for sentence in corpus for word in sentence.split()]\n",
    "    no_of_words = len(tokenized_corpus)\n",
    "    word_count = sum(sentence.lower().split().count(word.lower().strip('.,')) for sentence in corpus)\n",
    "    probability_word = word_count/no_of_words\n",
    "    print(probability_word)\n",
    "    \n",
    "    return probability_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "0.08333333333333333\n",
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_prob():\n",
    "    assert prob('this', corpus) == 0.25\n",
    "    assert prob('is', corpus) == 0.25\n",
    "    assert prob('sentence', corpus) == 0.25\n",
    "    assert round(prob('one', corpus),2) == 0.08\n",
    "    assert round(prob('two', corpus), 2) == 0.08\n",
    "    assert round(prob('three', corpus), 2) == 0.08\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one of the most important concepts in NLP is that of conditional probability. Conditional probability is the probability of an event occurring given that another event has already occurred. In the context of NLP, we can think of conditional probability as the probability of a word occurring given that another word has already occurred. For example, what is the probability of the word \"the\" occurring given that the word \"quick\" has already occurred? We can write this as P(the|quick).\n",
    "\n",
    "<b> Start by writing a function to return all bigrams in the corpus. The function should take a corpus as input and return a list of bigrams. A bigram is a tuple of two words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" contains 8 bigrams or co-occuring words: (The, quick), (quick, brown), (brown, fox), (fox, jumps), (jumps, over), (over, the), (the, lazy), (lazy, dog).\n",
    "\n",
    "We will also add start and end tokens to each sentence in the corpus. This is a common practice in NLP and is useful for computing probabilities. The start token will be represented by the string \"s\" and the end token by \"/s\". </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'one'), ('one', '</s>'), ('</s>', '<s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'two'), ('two', '</s>'), ('</s>', '<s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'three'), ('three', '</s>'))\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CORPUS #################\n",
    "corpus = ['<s> this is sentence one </s>',\n",
    "          '<s> this is sentence two </s>',\n",
    "          '<s> this is sentence three </s>']\n",
    "############################################\n",
    "from nltk import bigrams\n",
    "\n",
    "def find_bigrams1(corpus) -> list:\n",
    "    tokenized_corpus = [word.lower().strip('.,') for sentence in corpus for word in sentence.split()]\n",
    "    bi_grams = list(bigrams(tokenized_corpus))\n",
    "    \n",
    "    return bi_grams\n",
    "\n",
    "def find_bigrams(corpus) -> list:\n",
    "    \n",
    "    tokenized_corpus = [word.lower().strip('.,') for sentence in corpus for word in sentence.split()]\n",
    "    bi_grams = [tuple(zip(tokenized_corpus[:-1], tokenized_corpus[1:]))]\n",
    "    \n",
    "    return bi_grams[0]\n",
    "\n",
    "print(find_bigrams(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_bigrams():\n",
    "    assert find_bigrams(corpus)[0] == ('<s>', 'this')\n",
    "    assert find_bigrams(corpus)[1] == ('this', 'is')\n",
    "    assert find_bigrams(corpus)[2] == ('is', 'sentence')\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the bigrams, we can compute the probability of any word occurring after another as follows:\n",
    "\n",
    "$P(\\text{word2}|\\text{word1}) = \\frac{Count(\\text{word1}, \\text{word2})}{Count(\\text{word1})}$, where $Count(\\text{word1}, \\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together and $Count(\\text{word1})$ is the number of times $\\text{word1}$ occurs.\n",
    "\n",
    "For a clearer idea of why this formula works, refer page 4, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "<b>Write a function to compute the conditional probability of a word given another word. The function should take bigrams generated from the corpus and two words as input and return the conditional probability of the second word given the first word. For example, the probability of the word \"the\" occurring given that the word \"quick\" has already occurred is P(the|quick).</b>\n",
    "\n",
    "<i>Hint: You need to build the vocabulary set from the given bigrams inside the conditional probability function. </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'one'), ('one', '</s>'), ('</s>', '<s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'two'), ('two', '</s>'), ('</s>', '<s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'three'), ('three', '</s>'))\n"
     ]
    }
   ],
   "source": [
    "bigrams = find_bigrams(corpus)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(word:str, prev_word:str, bigrams:list) -> float:\n",
    "    required_bigram = (word, prev_word)\n",
    "    count_bigram = 0\n",
    "    count_word = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram == required_bigram:\n",
    "            count_bigram+=1\n",
    "        for word in bigram:\n",
    "            if word == prev_word:\n",
    "                count_word+=1\n",
    "                \n",
    "    cond_prob = count_bigram/count_word\n",
    "    return(cond_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conditional_prob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis\u001b[39m\u001b[38;5;124m'\u001b[39m, corpus)\n",
      "Cell \u001b[0;32mIn[236], line 12\u001b[0m, in \u001b[0;36mconditional_prob\u001b[0;34m(word, prev_word, bigrams)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m prev_word:\n\u001b[1;32m     10\u001b[0m             count_word\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 12\u001b[0m cond_prob \u001b[38;5;241m=\u001b[39m count_bigram\u001b[38;5;241m/\u001b[39mcount_word\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(cond_prob)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "conditional_prob('one', 'this', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good but you might be getting a division by zero error! If this is the case, let's use laplace smoothing to improve it. Laplace smoothing is a technique for dealing with words that do not occur in the corpus. It is a way of adjusting the probability of a word occurring given another word by adding 1 to the numerator and the number of unique words in the corpus to the denominator. This ensures that the probability of a word occurring is never 0. For a clearer idea of why this formula works, refer page 6, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "To implement laplace smoothing, we need to modify our conditional probability function as follows:\n",
    "\n",
    "$P\\left(\\text{word2}|\\text{word1} \\right) = \\frac{Count(\\text{word1},\\text{word2})+1}{Count(\\text{word1})+V}$ \n",
    "\n",
    "where $Count(\\text{word1},\\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together, $Count(\\text{word1})$, is the number of times $\\text{word1}$ occurs, and $V$ is the number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conditional_prob(word:str, prev_word:str, bigrams:list, k = 1) -> float:\n",
    "    # k is the smoothing constant and is set to 1 by default for laplace smoothing\n",
    "    # Find number of unique words\n",
    "    unique_words = {word for bigram in bigrams for word in bigram}\n",
    "    no_of_unique_words = len(unique_words)\n",
    "    \n",
    "    # Get the count of required bigram and the first word\n",
    "    required_bigram = (prev_word, word)\n",
    "    count_bigram = 0\n",
    "    count_word = 0\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram == required_bigram:\n",
    "            count_bigram+=1\n",
    "        if bigram[0] == prev_word:\n",
    "                count_word+=1            \n",
    "    \n",
    "    cond_prob = (count_bigram + k)/(count_word + no_of_unique_words)\n",
    "    return(cond_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_prob('is', 'this', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_conditional_prob():\n",
    "    assert round(conditional_prob('is', 'this', bigrams), 2) == 0.36\n",
    "    assert round(conditional_prob('sentence', 'this', bigrams), 2) == 0.09\n",
    "    assert round(conditional_prob('one', 'sentence', bigrams),2) == 0.18\n",
    "    assert round(conditional_prob('two', 'sentence', bigrams), 2) == 0.18\n",
    "    assert round(conditional_prob('three', 'sentence', bigrams), 2) == 0.18\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "\n",
    "test_conditional_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and predict the next word in a sentence. This is essentially what an advanced chatbot like ChatGPT also tries to do, although ChatGPT uses a much bigger corpus and a more sophisticated algorithm. We will use the same corpus as before, but this time we will use the conditional probability formula to predict the next word in a sentence. We will also use the start and end tokens to help us compute the probabilities.\n",
    "\n",
    "<b>Write a function to predict the next word in a sentence. The function should take a corpus and a sentence as input and return the most likely next word in the sentence. For example, if the sentence is \"the quick brown\", the function should return \"fox\".</b>\n",
    "\n",
    "<i>Hint: You can use the conditional probability function you wrote earlier to compute the probability of each word in the vocabulary occurring after the last word in the sentence. The word with the highest probability is the most likely next word. In other words, check the probability of each possible word in the corpus with respect to the given previous word and pick the one with the maximum probability. To make your algorithm more efficient, you can ignore non-co-occuring pairs when you build the vocabulary. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(prev_word: str, bigrams: list) -> str:\n",
    "    \n",
    "    # Get all words from bigram\n",
    "    all_words = [word for bigram in bigrams for word in bigram]\n",
    "    unique_words = set(all_words)\n",
    "    \n",
    "    # Define dict to store the word and associated conditional probability:\n",
    "    prob_dict ={}\n",
    "    for word in unique_words:\n",
    "        prob_dict[word] = conditional_prob(word, prev_word, bigrams)\n",
    "    \n",
    "    max_pair = max(prob_dict.items(), key=lambda x: x[1])\n",
    "    predicted_word = max_pair[0]\n",
    "    \n",
    "    return predicted_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word('this', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_predict_next_word():\n",
    "    assert predict_next_word('this', bigrams) == 'is'\n",
    "    assert predict_next_word('is', bigrams) == 'sentence'\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_predict_next_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and predict an entire sentence now.\n",
    "\n",
    "<b>Write a function that takes an initial string and a word limit as input and returns a sentence of the specified length. The initial string is the starting point for the sentence. For example, if the initial string is \"the quick brown\" and the word limit is 3, the function should return \"the quick brown fox jumps over\". The function should use the predict_next_word function you wrote earlier to predict the next word in the sentence.</b>\n",
    "\n",
    "<i>Hint: You can use a for loop to predict the next word in the sentence. The initial string is the starting point for the sentence. At each iteration, you can add the predicted word to the sentence and use the predicted word as the new initial string. You can stop the loop when the word limit is reached. </i>\n",
    "\n",
    "<i>Hint: You can use try, except to handle the case where the initial string is not in the corpus. If you don't catch the exception, you might encounter a value error. Try and add the word to your sentence, but if the 'predict_next_word' function fails, simply break the loop in the except block. This is not an ideal solution, but we will work with this for the purpose of this homework. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(input_sentence, bigrams, limit=2):\n",
    "    words = input_sentence.split()\n",
    "    last_word = input_sentence.split()[-1]\n",
    "    \n",
    "    for iteration in range(limit):\n",
    "        try:\n",
    "            predicted_word = predict_next_word(last_word, bigrams)\n",
    "            words.append(predicted_word)\n",
    "            last_word = predicted_word\n",
    "            if predicted_word == '</s>':\n",
    "                break\n",
    "        except ValueError:\n",
    "            break\n",
    "    sentence = ' '.join(words)\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is sentence one </s>'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"This is\", bigrams, limit = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_predict_sentence():\n",
    "    assert predict_sentence(\"This is\", bigrams, limit = 7) == 'This is sentence one </s>'\n",
    "    assert predict_sentence(\"This is\", bigrams, limit = 1) == 'This is sentence'\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "\n",
    "test_predict_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this with a real corpus. We will use the Brown corpus, which is a collection of 500 samples of English-language text, totaling roughly one million words. The Brown corpus is a good choice for this exercise because it is a balanced corpus, meaning it contains text from a variety of genres. This makes it a good representation of the English language. The Brown corpus is also a tagged corpus, meaning it contains information about the part of speech of each word. We will use this information later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/subashreedinesh/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# import the brown corpus from nltk\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "\n",
    "# download the corpus\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . </s>\", \"<s> The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . </s>\"]\n"
     ]
    }
   ],
   "source": [
    "# get the corpus\n",
    "corpus = brown.sents()\n",
    "\n",
    "corpus = [' '.join(sentence) for sentence in corpus]\n",
    "# Add start and end tags\n",
    "corpus = ['<s> ' + sentence + ' </s>' for sentence in corpus][:3000]\n",
    "print(corpus[0:2])\n",
    "bigrams_brown = find_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and predict a series of next words using the Brown corpus. The operation below might take a few seconds to run based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This  </s>'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"This\", bigrams_brown, limit = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A primer on perplexity\n",
    "Perplexity is a measurement in Natural Language Processing (NLP) used to evaluate language models. It's a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "The perplexity of a sentence is calculated as the inverse probability of the sentence, normalized by the number of words. In other words, it's the geometric mean of the inverse conditional probability of each word given the previous word in the sentence.\n",
    "\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_1, \\ldots, w_{i-1})}}$\n",
    "\n",
    "where $N$ is the number of words in the sentence and $P(w_i \\mid w_1, \\ldots, w_{i-1})$ is the conditional probability of the i'th word given the previous words in the sentence.\n",
    "\n",
    "In our case, since we only have bigrams, the formula becomes:\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_{i-1})}}$\n",
    "\n",
    "Perplexity is a useful metric for evaluating language models because it is a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "Let's try and calculate the perplexity for a few sentences in the Brown corpus. We will use the same corpus as before, but this time we will use the perplexity formula to calculate the perplexity of each sentence. We will also use the start and end tokens to help us compute the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentence, bigrams) -> float:\n",
    "    # your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pick 5 sentences from the Brown corpus and calculate the perplexity of each sentence. The code to import and load the corpus is provided above. Also calculate the average perplexity of the subset of the brown corpus chosen above.\n",
    "\n",
    "You can also make up your own sentences and calculate the perplexity of each sentence. How does the perplexity of your sentences compare to the perplexity of the sentences in the Brown corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705.8333187321628"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "perplexity(\"This is \", bigrams_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_perplexity():\n",
    "    corpus = ['<s> this is sentence one </s>',\n",
    "              '<s> this is sentence two </s>',\n",
    "              '<s> this is sentence three </s>']\n",
    "    bigrams = find_bigrams(corpus)\n",
    "    perplexity_score = perplexity(\"This is \", bigrams)\n",
    "    assert round(perplexity_score, 2) == 2.83\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a brief half-page report on the concepts you explored in this exercise. You can use the questions below as a guide.\n",
    "1) What is the conditional probability of a word occurring given its previous word and how does this probability change when you use laplace smoothing?\n",
    "2) What do you expect will happen when you use trigrams instead of bigrams to predict the next word in a sentence? Does the perplexity increase or decrease? Why?\n",
    "3) What do you expect will happen when you use a larger corpus to predict the next word in a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! You've built yourself a next word predictor. Now go put it out there and get acquired by Microsoft for a few billion dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further exercises (will not be graded):\n",
    "\n",
    "- Try to improve the model by using trigrams instead of bigrams. A trigram is a tuple of three words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" contains 7 trigrams: (The, quick, brown), (quick, brown, fox), (brown, fox, jumps), (fox, jumps, over), (jumps, over, the), (over, the, lazy), (the, lazy, dog). You can use the same formula to compute the conditional probability of a word given two other words. For example, the probability of the word \"the\" occurring given that the words \"quick\" and \"brown\" have already occurred is P(the|quick, brown). You can also use the start and end tokens to help you compute the probabilities. For example, the probability of the word \"the\" occurring given that the words \"s\" and \"quick\" have already occurred is P(the|s, quick). You can use the same function you wrote earlier to compute the conditional probability of a word given two other words. The function should take a corpus and three words as input and return the conditional probability of the third word given the first two words. For example, the probability of the word \"the\" occurring given that the words \"quick\" and \"brown\" have already occurred is P(the|quick, brown)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
